{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir images\n",
    "!touch images/missing_pictures.txt\n",
    "!wget https://github.com/metmuseum/openaccess/raw/master/MetObjects.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Egyptian Museum\n",
    "The goal of this experiment is to determine whether we can use machine learning to date Egyptian artifacts using only their photographs.\n",
    "\n",
    "The artifacts are very varied in their appearance, material used and size. We will only focus on the appearance for now."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pandas.read_csv('MetObjects.csv')\n",
    "egypt = df[df['Department']=='Egyptian Art']\n",
    "egypt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting images\n",
    "Here we download all possible images of the egyptian artifacts.\n",
    "\n",
    "# !!!!! This process might be interrupted by the server. If that happens, re-run this cell. !!!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "def download_image_for_id(obj_id):\n",
    "    response = json.loads(requests.get(url=f'https://collectionapi.metmuseum.org/public/collection/v1/objects/{obj_id}').content)\n",
    "    if not \"primaryImageSmall\" in response or not response['primaryImageSmall']:\n",
    "        print(obj_id, file=open(\"images/missing_pictures.txt\", \"a+\"))\n",
    "        return\n",
    "    image_response = requests.get(url=response['primaryImageSmall'])\n",
    "    if image_response.status_code != 200:\n",
    "        print(obj_id, file=open(\"images/missing_pictures.txt\", \"a+\"))\n",
    "        return\n",
    "    image_bytes = image_response.content\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    image.save(f\"images/{obj_id}.jpg\")\n",
    "\n",
    "all_ids = egypt['Object ID'].values\n",
    "with open(\"images/missing_pictures.txt\", \"r\") as f:\n",
    "    known_missing_pictures = [int(line) for line in f]\n",
    "missing_ids = [obj_id for obj_id in all_ids if not Path(f\"images/{obj_id}.jpg\").exists() and obj_id not in known_missing_pictures]\n",
    "if missing_ids:\n",
    "    with Pool(10) as pool:\n",
    "        _ = list(tqdm(pool.imap(download_image_for_id, missing_ids), total=len(missing_ids)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preprocessing:\n",
    "The museum contains Egyptian artifacts ranging from the Neolithic to modern era. We will only focus on the dynastic period, ~3500 BCE - 30BCE (Roman annexation of Egypt).\n",
    "* All pictures will be resized to 224, 224 (as AlexNet was trained on).\n",
    "* Some pictures are originally greyscale. These will be treated as colored pictures nonetheless.\n",
    "* Start and end dates will be normalized (we make the big assumption that the distribution of dates is gaussian)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(egypt[(egypt['Object Begin Date']>-3500) & (egypt['Object End Date']<0)]['Object Begin Date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "egypt_dynastic = egypt[(egypt['Object Begin Date']>-3500) & (egypt['Object End Date']<0)]\n",
    "egypt_with_pictures = egypt_dynastic[egypt_dynastic['Object ID'].apply(lambda obj_id: Path(f'images/{obj_id}.jpg').exists())]\n",
    "random.seed(147)\n",
    "val_obj_ids = random.sample(list(egypt_with_pictures['Object ID'].values), int(len(egypt_with_pictures)*0.1))\n",
    "train_dataset, val_dataset = [], []\n",
    "class UnGreyScale(torch.nn.Module):\n",
    "    def forward(self, tensor):\n",
    "        if tensor.shape[0] == 3: return tensor\n",
    "        elif tensor.shape[0] == 1: return tensor.repeat(3, 1, 1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    UnGreyScale(),\n",
    "])\n",
    "total_epoch_mean = egypt_with_pictures[['Object Begin Date', 'Object End Date']].values.mean()\n",
    "total_epoch_std = egypt_with_pictures[['Object Begin Date', 'Object End Date']].values.std()\n",
    "\n",
    "for obj_id, min_year, max_year in tqdm(egypt_with_pictures[['Object ID', 'Object Begin Date', 'Object End Date']].values):\n",
    "    image = Image.open(f'images/{obj_id}.jpg')\n",
    "    image = transform(image)\n",
    "    min_year_norm = (min_year - total_epoch_mean) / total_epoch_std\n",
    "    max_year_norm = (max_year - total_epoch_mean) / total_epoch_std\n",
    "    if obj_id in val_obj_ids:\n",
    "        val_dataset.append({'inputs':image, 'min_date':min_year_norm, 'max_date':max_year_norm})\n",
    "    else:\n",
    "        train_dataset.append({'inputs':image, 'min_date':min_year_norm, 'max_date':max_year_norm})\n",
    "seed_everything(147, workers=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "Model is based on the AlexNet, a pre-trained computer vision model that was trained on the ImageNet dataset.\n",
    "\n",
    "We add our own regression layer at the end, and set learning rates. We fine-tune also the AlexNet's parameters.\n",
    "\n",
    "The loss function is the MSE, counted from the center of the time interval the artifact was dated to. Experiments with tweaked MSE were made, but those did not improve performance.\n",
    "\n",
    "A prediction is considered accurate, if it falls in the interval the artifact was dated to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def interval_loss(label_min, label_max, pred):\n",
    "#     label_mid_dist =  (pred - (label_min + label_max) / 2) ** 2\n",
    "#     lower_loss = (pred < label_min) * label_mid_dist\n",
    "#     upper_loss = (pred > label_max) * label_mid_dist\n",
    "#     middle_loss = ((label_max > pred) & (pred > label_min)) * label_mid_dist\n",
    "#     return torch.mean(lower_loss + upper_loss + 0.2 * middle_loss)\n",
    "\n",
    "def accuracy(label_min, label_max, pred):\n",
    "    return ((label_max>=pred)&(pred>=label_min)).sum()/label_min.shape[0]\n",
    "\n",
    "class DateClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.alexnet(pretrained=True)\n",
    "        regressor = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(9216, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 1),\n",
    "        )\n",
    "        #self.model.requires_grad = False\n",
    "        # last_ftrs = self.model.fc.in_features\n",
    "        # last_layer = torch.nn.Linear(last_ftrs, 1)\n",
    "        self.model.classifier = regressor\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {\n",
    "                'params':[param for name, param in self.model.named_parameters() if not name.startswith(\"model.classifier\")],\n",
    "                'lr': 1e-4\n",
    "            },\n",
    "            {\n",
    "                'params':[param for name, param in self.model.named_parameters() if name.startswith('model.classifier')],\n",
    "                'lr':1e-2\n",
    "             }\n",
    "        ])\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        outs = self(batch['inputs']).squeeze()\n",
    "        # loss = interval_loss(batch['min_date'], batch['max_date'], outs)\n",
    "        loss = torch.nn.functional.mse_loss(outs, (batch['min_date'] + batch['max_date']).float()/2)\n",
    "        acc = accuracy(batch['min_date'], batch['max_date'], outs)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_accuracy', acc)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.eval()\n",
    "        outs = self(batch['inputs']).squeeze()\n",
    "        # loss = interval_loss(batch['min_date'], batch['max_date'], outs)\n",
    "        loss = torch.nn.functional.mse_loss(outs, (batch['min_date'] + batch['max_date']).float()/2)\n",
    "        acc = accuracy(batch['min_date'], batch['max_date'], outs)\n",
    "        self.log('val_loss',loss)\n",
    "        self.log('val_accuracy',acc)\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.eval()\n",
    "        outs = self(batch['inputs']).squeeze()\n",
    "        acc = accuracy(batch['min_date'], batch['max_date'], outs)\n",
    "        self.log('test_accuracy',acc)\n",
    "        return acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "lightning_model = DateClassifier()\n",
    "trainer = Trainer(gpus=-1 if torch.cuda.is_available() else 0, precision=16, max_epochs=100)\n",
    "trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "trainer.test(lightning_model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "Evaluation can be done using tensorboard, as in the cell below.\n",
    "\n",
    "## Comments\n",
    "My home experiments yielded a very disappointing accuracy of ~21%. This however could be most certainly improved, given enough time. A few areas of possible improvement come to mind:\n",
    "* The pictures were not cropped in any way, and the backgrounds on them are vastly different from each other.\n",
    "* There was no pre-processing transformation of the pictures (e.g. rotation, distortion, mirroring...)\n",
    "* The dataset contains information on the materials used and the sizes of the artifacts. That could also possibly help.\n",
    "* Archeologicaly very important information like the name of the site the artifact was found in was also not used.\n",
    "* More experiments with tuning hyperparameters and loss functions could be made."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./lightning_logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
